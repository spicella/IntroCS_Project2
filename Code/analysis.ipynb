         # function that plots proba dist

def prbplt(x):

    s = x.sum(axis = 0, skipna = True)
    f = sorted(s.value_counts()/5242, reverse = True)


    #Plotting (log-log) graphs

    plt.figure(figsize=(20,10))
    plt.title("Probabablity of Authors having degree k",fontsize=20)
    plt.plot(f, linewidth = 1, marker = "o", markersize = 5,ls="-.")
    plt.xlabel("Log k", fontsize = 18)
    plt.ylabel("Log Probability", fontsize = 18)

    #Comment out the following 3 lines to obtain "non-log" graph, i.e. "Probability vs. k" graph
#     plt.semilogx()
#     plt.semilogy()
#     plt.grid()
    
    return plt.show()

#Function calculates: Average Local CC
#Input info: x = adj_a (original dataframe matrix)

def CC(x):
    
    adj_mat = np.asarray(x)     #converts dataframe matrix to numpy matrix array


    tot_cc = 0
    k=np.sum(adj_mat,axis=0)

    for i in range(0,len(adj_mat)):

        p = []
        for u in range(0,len(adj_mat)):
            if adj_mat[i,u] == 1:
                p.append(u)
            else:
                p = p            #p -> gives list of connected neighbours of a node

        L = 0
        for q in range (0, len(p)):
            for h in range (0,len(p)):
                if adj_mat[p[q], p[h]] == 1:
                    L = L + 1
                else:
                    L = L        #L -> gives 2x number of triangles connected to given node (double counting)
            
        if k[i]==1:
            cc=0
        else:
            cc = L/(k[i]*(k[i]-1))
        tot_cc = (tot_cc + cc)

    
    print("Average CC is: ", tot_cc/len(adj_mat))


#functions for the statistic tests
def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return idx

def powpow(x,a,x_min):
    if a<0:
        print("only negative powers allowed! (stoopid)") 
    if x_min<=0:
        print("only positive x_min allowed! (stoopid)")
    else:
        return (a-1)/(x_min) * 1/np.power(x/x_min,a)
    
def fitPow(adj_data):
    
    s = adj_data.sum(axis = 0, skipna = True)
    f_unique_elements, f_counts_elements = np.unique(s, return_counts=True)
    observed = f_counts_elements/sum(f_counts_elements)
    x = f_unique_elements
    x_min=2
    cut = find_nearest(x,x_min)
    x = x[cut:]
    observed = observed[cut:]
    
    len_x=len(x)
    #estimate parameter alpha
    ln_x=np.log(x)
    alpha=len_x/sum(ln_x)+1
    expected = powpow(x, alpha,x_min)
    chi_stat, p_val = chisquare(observed, expected)
    print(chi_stat, p_val, alpha)
    plt.plot(x,observed,label="Cutted Data @ x_min=%d"%(x_min),marker='s',ls='--',color='limegreen')    

    plt.plot(x,powpow(x,alpha,x_min),label="Power law Distribution fit, p=%.3f"%(p_val),color='r')

    plt.legend()
    print(x)
    

def fitPois(adj_data):
    s = adj_data.sum(axis = 0, skipna = True)
    f_unique_elements, f_counts_elements = np.unique(s, return_counts=True)
    observed = f_counts_elements/sum(f_counts_elements)
    x = f_unique_elements
    lbd=np.mean(s)
    expected = poisson.pmf(x, lbd, 0)
    chi_statistic, p_value = chisquare(observed, expected)
    print("%.10f, %.10f, %.10f"%(chi_statistic, p_value, lbd))
    plt.figure(figsize=(16,9))
    plt.title("hi")#propriet rete
    plt.ylim(0, max(observed)*1.1)
    plt.plot(x,observed,label="Original data",markersize=10,alpha=.7,marker='o',color='coral',ls='--')
    plt.plot(x,poisson.pmf(x,lbd,0),label="Poisson Distribution fit, p=%.3f"%(p_value))
    plt.legend(fontsize=12)





