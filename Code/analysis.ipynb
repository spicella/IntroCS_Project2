         # function that plots proba dist

def prbplt(x):

    s = x.sum(axis = 0, skipna = True)
    f = sorted(s.value_counts()/5242, reverse = True)


    #Plotting (log-log) graphs

    plt.figure(figsize=(20,10))
    plt.title("Probabablity of Authors having degree k",fontsize=20)
    plt.plot(f, linewidth = 1, marker = "o", markersize = 5,ls="-.")
    plt.xlabel("Log k", fontsize = 18)
    plt.ylabel("Log Probability", fontsize = 18)

    #Comment out the following 3 lines to obtain "non-log" graph, i.e. "Probability vs. k" graph
#     plt.semilogx()
#     plt.semilogy()
#     plt.grid()
    
    return plt.show()

#Function calculates: Average Local CC
#Input info: x = adj_a (original dataframe matrix)

def CC(x):
    
    adj_mat = np.asarray(x)     #converts dataframe matrix to numpy matrix array


    tot_cc = 0
    k=np.sum(adj_mat,axis=0)

    for i in range(0,len(adj_mat)):

        p = []
        for u in range(0,len(adj_mat)):
            if adj_mat[i,u] == 1:
                p.append(u)
            else:
                p = p            #p -> gives list of connected neighbours of a node

        L = 0
        for q in range (0, len(p)):
            for h in range (0,len(p)):
                if adj_mat[p[q], p[h]] == 1:
                    L = L + 1
                else:
                    L = L        #L -> gives 2x number of triangles connected to given node (double counting)
            
        if k[i]==1:
            cc=0
        else:
            cc = L/(k[i]*(k[i]-1))
        tot_cc = (tot_cc + cc)

    
    print("Average CC is: ", tot_cc/len(adj_mat))


#functions for the statistic tests NB: call first fitPois and then fitPow for a unique plot

#given a value s and an array, finds the index of the element which is the nearest to s
def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return idx

#selfmade power law distribution (normalized), with support (x_min,+inf)
def powpow(x,a,x_min): #selfmade power law distribution (normalized), with support (x_min,+inf)
    if a<0:
        print("only negative powers allowed! (stoopid)") 
    if x_min<=0:
        print("only positive x_min allowed! (stoopid)")
    else:
        return (a-1)/(x_min) * 1/np.power(x/x_min,a)


    
# fitPois estimes the parameter lambda of the power law distriburion with ML method and does a goodness of fit test  
# NB: x_max must be inserted
def fitPois(adj_data):
    
    # observed relative frequencies are calculated and estime of lambda
    s = adj_data.sum(axis = 0, skipna = True)
    lbd=np.mean(s)
    f_unique_elements, f_counts_elements = np.unique(s, return_counts=True)
    observed = f_counts_elements/sum(f_counts_elements)
    x = f_unique_elements
    
    #cut-off of the tail: all data grater then x_max are included in the same class in the chisquared test
    x_max=20
    cut = find_nearest(x,x_max)
    observedcut = np.append(observed[0:cut],observed[cut:].sum())
    
    #expected frequencies are computed and chi squared test is performed
    expected = poisson.pmf(x, lbd, 0)
    expectedcut=np.append(expected[0:cut],expected[cut:].sum())
    chi_statistic, p_value = chisquare(observedcut, expectedcut)
    
    #plots
    print("Poisson Distribution fit: chi_squared=%.10f, p=%.10f,lambda=%.10f"%(chi_statistic, p_value, lbd))
    plt.figure(figsize=(16,9))
    plt.title("Distribution of ... ")#propriet rete
    plt.ylim(0, max(observed)*1.1)
    plt.plot(x,observed,label="Original data",markersize=10,alpha=.7,marker='o',color='coral',ls='--')
    plt.plot(x,poisson.pmf(x,lbd,0),label="Poisson Distribution fit, p=%.3f"%(p_value),color='royalblue')
    plt.legend(fontsize=12)

# fitPow estimes the parameter alpha of the power law distriburion with ML method and does a goodness of fit test 
# NB: x_min is not estimated but must be inserted
def fitPow(adj_data):
    
    # observed relative frequences are calculated
    s = adj_data.sum(axis = 0, skipna = True)
    f_unique_elements, f_counts_elements = np.unique(s, return_counts=True)
    observed = f_counts_elements/sum(f_counts_elements)
    
    #cut-off of first observations up to x_min 
    x_min=2 
    x = f_unique_elements
    cut = find_nearest(x,x_min)
    x = x[cut:]
    observed = observed[cut:]
    
    #estime of parameter alpha
    len_x=len(x)
    ln_x=np.log(x)
    alpha=len_x/sum(ln_x)+1
    
    #expected relative frequencies are calculated and chisquared test is performed 
    expected = powpow(x, alpha,x_min)
    chi_stat, p_val = chisquare(observed, expected)
    
    #plots
    print("Power law Distribution fit: chi_squared=%.10f, p=%.10f, alpha=%.10f"%(chi_stat, p_val, alpha))
    plt.plot(x,observed,label="Cutted Data @ x_min=%d"%(x_min),marker='s',ls='--',color='limegreen')    
    plt.plot(x,powpow(x,alpha,x_min),label="Power law Distribution fit, p=%.3f"%(p_val),color='r')
    plt.legend()


